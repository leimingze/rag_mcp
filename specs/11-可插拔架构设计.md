## 可插拔架构设计
目标： 定义清晰的抽象层与接口契约，使 RAG 链路的每个核心组件都能够独立替换与升级，避免技术锁定，支持低成本的 A/B 测试与环境迁移。
```bash
术语说明：本节中的"提供者 (Provider)"、"实现 (Implementation)"指的是完成某项功能的具体技术方案，而非传统 Web 架构中的"后端服务器"。例如，LLM 提供者可以是远程的 Azure OpenAI API，也可以是本地运行的 Ollama；向量存储可以是本地嵌入式的 Chroma，也可以是云端托管的 Pinecone。本项目作为本地 MCP Server，通过统一接口对接这些不同的提供者，实现灵活切换。
```
### 设计原则
- 接口隔离 (Interface Segregation)：为每类组件定义最小化的抽象接口，上层业务逻辑仅依赖接口而非具体实现。
- 配置驱动 (Configuration-Driven)：通过统一配置文件（如 settings.yaml）指定各组件的具体后端，代码无需修改即可切换实现。
- 工厂模式 (Factory Pattern)：使用工厂函数根据配置动态实例化对应的实现类，实现"一处配置，处处生效"。
- 优雅降级 (Graceful Fallback)：当首选后端不可用时，系统应自动回退到备选方案或安全默认值，保障可用性。
通用结构示意：
```bash
业务代码
  │
  ▼
<Component>Factory.get_xxx()  ← 读取配置，决定用哪个实现
  │
  ├─→ ImplementationA()
  ├─→ ImplementationB()  
  └─→ ImplementationC()
      │
      ▼
    都实现了统一的抽象接口
```

### LLM 与 Embedding 提供者抽象
这是可插拔设计的核心环节，因为模型提供者的选择直接影响成本、性能与隐私合规。
- 统一接口层 (Unified API Abstraction)：
  - 设计思路：无论底层使用 Azure OpenAI、OpenAI 原生 API、DeepSeek 还是本地 Ollama，上层调用代码应保持一致。
  - 关键抽象：
    - LLMClient：暴露 chat(messages) -> response 方法，屏蔽不同 Provider 的认证方式与请求格式差异。
    - EmbeddingClient：暴露 embed(texts) -> vectors 方法，统一处理批量请求与维度归一化。
- 技术选型建议：对于企业级需求，可在其基础上增加统一的 重试、限流、日志 中间层，提升生产可靠性。
- Vision LLM 扩展：针对图像描述生成（Image Captioning）需求，系统扩展了 BaseVisionLLM 接口，支持文本+图片的多模态输入。当前实现：
  - Azure OpenAI Vision（GPT-4o/GPT-4-Vision）：企业级合规部署，支持复杂图表解析，与 Azure 生态深度集成。

### 检索策略抽象
检索层的可插拔性决定了系统在不同数据规模与查询模式下的适应能力。
设计模式：抽象工厂模式

### 评估框架抽象
- 设计思路
  - 定义统一的 Evaluator 接口，暴露 evaluate(query, retrieved_chunks, generated_answer, ground_truth) -> metrics 方法。
  - 各评估框架实现该接口，输出标准化的指标字典。
  - RAG 评估框架对比

| 框架 | 特点 | 适用场景 |
|------|------|----------|
| Ragas | RAG 专用，指标丰富（Faithfulness、Answer Relevancy、Context Precision 等） | 全面评估 RAG 质量、学术对比 |
| DeepEval | LLM-as-Judge 模式，支持自定义评估标准 | 需要主观质量判断、复杂业务规则 |
| 自定义指标 | Hit Rate、MRR、Latency P99 等基础工程指标 | 快速回归测试、上线前 Sanity Check |

  - 组合与扩展：
    - 评估模块设计为组合模式，可同时挂载多个 Evaluator，生成综合报告。
    - 配置示例：evaluation.backends: [ragas, custom_metrics]，系统并行执行并汇总结果。
### 配置管理与切换流程
- 配置文件结构示例 (config/settings.yaml)：
```yaml
 llm:
   provider: azure  # azure | openai | ollama | deepseek
   model: gpt-4o
   # provider-specific configs...
 
 embedding:
   provider: openai
   model: text-embedding-3-small
 
 vector_store:
   backend: chroma  # chroma | qdrant | pinecone
 
 retrieval:
   sparse_backend: bm25  # bm25 | elasticsearch
   fusion_algorithm: rrf  # rrf | weighted_sum
   rerank_backend: cross_encoder  # none | cross_encoder | llm
 
 evaluation:
   backends: [ragas, custom_metrics]
```
